{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcc_traffic_analysis.testing import TestingPipeline\n",
    "from tcc_traffic_analysis.custom_algorithms import CustomAlgorithm\n",
    "from tcc_traffic_analysis.custom_algorithms import ManualFeatureSelection\n",
    "from tcc_traffic_analysis.classification import ClassificationPipeline\n",
    "from tcc_traffic_analysis.datasets import CustomDataSet\n",
    "\n",
    "from tcc_traffic_analysis.classification import run_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from datetime import date\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifiers():\n",
    "    classificadores = []\n",
    "    clf = (\"Gaussian Naive Bayes\", GaussianNB(), {})\n",
    "    classificadores.append(clf)\n",
    "    clf = (\"Knn\", KNeighborsClassifier(), {})\n",
    "    classificadores.append(clf)\n",
    "    clf = ('MLP', MLPClassifier(), {})\n",
    "    classificadores.append(clf)\n",
    "    clf = (\"XGBoost\", XGBClassifier(), {})\n",
    "    classificadores.append(clf)\n",
    "    clf = (\"Linear Discriminant Analysis\", LinearDiscriminantAnalysis(), {})\n",
    "    classificadores.append(clf)\n",
    "    clf = (\"Quadratic Discriminant Analysis \", QuadraticDiscriminantAnalysis(), {})\n",
    "    classificadores.append(clf)\n",
    "    clf = (\"Decision Trees\", DecisionTreeClassifier(), {})\n",
    "    classificadores.append(clf)\n",
    "    clf = (\"Random Forests\", RandomForestClassifier(), {})\n",
    "    classificadores.append(clf)\n",
    "    clf = (\"Gradient Boosting\", GradientBoostingClassifier(), {})\n",
    "    classificadores.append(clf)\n",
    "    clf = (\"Ada Boosting\", AdaBoostClassifier(), {})\n",
    "    classificadores.append(clf)\n",
    "    return classificadores\n",
    "\n",
    "def create_scorers():\n",
    "    scorers = {'accuracy_score': make_scorer(accuracy_score),\n",
    "               'precision_score': make_scorer(precision_score, average='micro'),\n",
    "               'f1_score': make_scorer(f1_score, average='micro'),\n",
    "               'recall_score': make_scorer(recall_score, average='micro')}\n",
    "    return scorers\n",
    "\n",
    "def generate_test_dataset_list(dataset_path, dataset_name_std, dataset_id, ext=\".csv\", num_datasets=5, path_in_name=False):\n",
    "    tests = []\n",
    "    for i in range(0, num_datasets):\n",
    "        if path_in_name:\n",
    "            tests.append(dataset_path + dataset_name_std + str(dataset_id + i) + ext)\n",
    "        else:\n",
    "            tests.append(dataset_name_std + str(dataset_id + i) + ext)\n",
    "    return tests\n",
    "\n",
    "def generate_manual_feature_selection_method():\n",
    "    metrics           = [\"Mutual information\", \"Importance\", \"Abs Correlation\"]\n",
    "    n_features_keeps  = [10, 20, 30, 40]\n",
    "    result_control_id = 1\n",
    "    jump_control      = 5\n",
    "    result = []\n",
    "    for n_features_keep in n_features_keeps:\n",
    "        for metric in metrics:\n",
    "            if result_control_id > jump_control:\n",
    "                fs = CustomAlgorithm(metric + \" \" + str(n_features_keep), ManualFeatureSelection(metric, n_features_keep))\n",
    "                result.append(fs)\n",
    "                result_control_id += 1\n",
    "            else:\n",
    "                result_control_id += 1\n",
    "    return result\n",
    "                \n",
    "def generate_feature_selection_algorithms():\n",
    "    fs_1 = CustomAlgorithm(\"Tree\", SelectFromModel(ExtraTreesClassifier()))\n",
    "    fs_2 = CustomAlgorithm(\"PCA\" , PCA())\n",
    "    manual = generate_manual_feature_selection_method()\n",
    "    return [fs_1, fs_2] + manual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_class_tor_non_tor_transform(dataset, column_name, rows=None):\n",
    "    # Iot 1, Non Iot 0\n",
    "    if rows is not None:\n",
    "        return dataset[column_name].apply(lambda x: 0 if x in [\"Non Tor\", \"Mobile\"] else 1).iloc[rows]\n",
    "    else:\n",
    "        return dataset[column_name].apply(lambda x: 0 if x in [\"Non Tor\", \"Mobile\"] else 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_class_tor_non_tor_iot_class_transform(dataset, column_name, rows=None):\n",
    "    # Iot 1, Non Iot 0\n",
    "    if rows is not None:\n",
    "        return dataset[column_name].apply(lambda x: 0 if x in [\"Non Tor\", \"Mobile\"] else 1).iloc[rows]\n",
    "    else:\n",
    "        return dataset[column_name].apply(lambda x: 0 if x in [\"Non Tor\", \"Mobile\"] else 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_class_anom_transform(dataset, column_name, rows=None):\n",
    "    # Benigno 0, Anomalia 1\n",
    "    if rows is not None:\n",
    "        return dataset[column_name].apply(lambda x: 0 if \"Anomaly\" not in x else 1).iloc[rows]\n",
    "    else:\n",
    "        return dataset[column_name].apply(lambda x: 0 if \"Anomaly\" not in x else 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTO 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experimento_1():\n",
    "    datasets = [(\"./datasets/experimento-1/iot-mobile-nontor/\", \"05_05_05-\"),\n",
    "                  (\"./datasets/experimento-1/iot-mobile/\", \"05_10_00-\"),\n",
    "                  (\"./datasets/experimento-1/iot-nontor/\", \"05_00_10-\")]\n",
    "    dataset_ids = [90]\n",
    "    num_datasets = 5\n",
    "    test_size = 0.3\n",
    "    today = date.today().strftime('%d-%m-%Y')\n",
    "    experimento = '1'\n",
    "    final_ds = pd.DataFrame()\n",
    "\n",
    "    for dataset_id in dataset_ids:\n",
    "        files_to_test = []\n",
    "        for train_dataset in datasets:\n",
    "            dataset_path = train_dataset[0]\n",
    "            dataset_name_std = train_dataset[1]\n",
    "            files_to_test += generate_test_dataset_list(dataset_path, dataset_name_std, dataset_id, path_in_name=True, ext=\"-testcleaned.csv\")\n",
    "        for train_dataset in datasets:\n",
    "            dataset_path = train_dataset[0]\n",
    "            dataset_name_std = train_dataset[1]\n",
    "            print(\"Doing\", dataset_path + dataset_name_std)\n",
    "            for i in range(0, num_datasets):\n",
    "                print(\"\\t\", i)\n",
    "                new_dataset_id = dataset_id + i\n",
    "                dataset_file = dataset_path + dataset_name_std + str(new_dataset_id) + \"-traincleaned.csv\"\n",
    "\n",
    "                feature_selection_methods = generate_feature_selection_algorithms()\n",
    "                classifiers = create_classifiers()\n",
    "                scorers = create_scorers()\n",
    "\n",
    "                tests = TestingPipeline(None, files_to_test, y_function=device_class_tor_non_tor_transform, name_in_path=True)\n",
    "                df = CustomDataSet(dataset_name_std + str(new_dataset_id) + \"-traincleaned.csv\", dataset_file, test_size=test_size, y_function=device_class_tor_non_tor_transform)\n",
    "                print(\"\\t\", dataset_file)\n",
    "                run_classifiers(df, experimento, tests, scorers, classifiers, feature_selection_methods)\n",
    "            \n",
    "                final_ds = pd.concat([final_ds, tests.return_dataframe()])\n",
    "            final_ds.to_csv(\"./resultados/experimento-\" + experimento + \"/resultado_\" + \"experimento_\" + experimento + \"-\" + str(dataset_id) + \"-\" + today + \".csv\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experimento_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTO 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experimento_2(case_base=False):\n",
    "    train_datasets = [(\"./datasets/experimento-2/all_iot/\", \"all_iot_05_05_05-\")]\n",
    "    test_datasets = [\n",
    "                   (\"./datasets/experimento-2/appliences/\", \"appliences_05_05_05-\"),\n",
    "                   (\"./datasets/experimento-2/cameras/\", \"cameras_05_05_05-\"),\n",
    "                   (\"./datasets/experimento-2/controller_hubs/\", \"controller-hubs_05_05_05-\"),\n",
    "                  (\"./datasets/experimento-2/energy_managment/\", \"energy-managment_05_05_05-\"),\n",
    "                   (\"./datasets/experimento-2/health_monitor/\", \"health-monitor_05_05_05-\")\n",
    "    ]\n",
    "    if not case_base:\n",
    "        train_datasets = test_datasets\n",
    "\n",
    "    dataset_ids = [100]\n",
    "    num_datasets = 5\n",
    "    test_size = 0.3\n",
    "    today = date.today().strftime('%d-%m-%Y')\n",
    "    experimento = '2'\n",
    "    final_ds = pd.DataFrame()\n",
    "\n",
    "    print(\"Experimento 2\")\n",
    "    print(train_datasets)\n",
    "    for dataset_id in dataset_ids:\n",
    "        files_to_test = []\n",
    "        for train_dataset in train_datasets:\n",
    "            if not case_base:\n",
    "                dataset_test_path = train_dataset[0]\n",
    "                dataset_test_name_std = train_dataset[1]\n",
    "                files_to_test += generate_test_dataset_list(dataset_test_path, dataset_test_name_std, dataset_id, path_in_name=True, ext=\"-test.csv\") \n",
    "            else:\n",
    "                for test_dataset in test_datasets:\n",
    "                    dataset_test_path = test_dataset[0]\n",
    "                    dataset_test_name_std = test_dataset[1]\n",
    "                    files_to_test += generate_test_dataset_list(dataset_test_path, dataset_test_name_std, dataset_id, path_in_name=True, ext=\"-test.csv\")\n",
    "            dataset_path = train_dataset[0]\n",
    "            dataset_name_std = train_dataset[1]\n",
    "            print(\"Doing\", dataset_path + dataset_name_std)\n",
    "            for i in range(0, num_datasets):\n",
    "                print(\"\\t\", i)\n",
    "                new_dataset_id = dataset_id + i\n",
    "                dataset_file = dataset_path + dataset_name_std + str(new_dataset_id) + \"-train.csv\"\n",
    "            \n",
    "                feature_selection_methods = generate_feature_selection_algorithms()\n",
    "                classifiers = create_classifiers()\n",
    "                scorers = create_scorers()\n",
    "            \n",
    "                tests = TestingPipeline(dataset_path, files_to_test, y_function=device_class_tor_non_tor_transform, name_in_path=True)\n",
    "                df = CustomDataSet(dataset_name_std + str(new_dataset_id) + \"-trains.csv\", dataset_file, test_size=test_size, y_function=device_class_tor_non_tor_transform)\n",
    "                run_classifiers(df, experimento, tests, scorers, classifiers, feature_selection_methods)\n",
    "            \n",
    "                final_ds = pd.concat([final_ds, tests.return_dataframe()])\n",
    "            if case_base:\n",
    "                final_ds.to_csv(\"./resultados/experimento-\" + experimento + \"/resultado_\" + \"experimento_\" + experimento + \"_base_hip-\" + str(dataset_id) + \"-\" + today + \".csv\")\n",
    "            else:\n",
    "                final_ds.to_csv(\"./resultados/experimento-\" + experimento + \"/resultado_\" + \"experimento_\" + experimento + \"-\" + str(dataset_id) + \"-\" + today + \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experimento_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experimento_2(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTO EXTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./datasets/experimento-extra/\"\n",
    "dataset_name_std = \"anomaly_05_05_05-\"\n",
    "dataset_ids = [90]\n",
    "num_datasets = 5\n",
    "test_size = 0.3\n",
    "today = date.today().strftime('%d-%m-%Y')\n",
    "experimento = 'extra'\n",
    "final_ds = pd.DataFrame()\n",
    "\n",
    "\n",
    "for dataset_id in dataset_ids:\n",
    "    print(\"Doing\",dataset_path + dataset_name_std)\n",
    "    for i in range(0, num_datasets):\n",
    "        print(\"\\t\", i)\n",
    "        new_dataset_id = dataset_id + i\n",
    "        dataset_file = dataset_path + dataset_name_std + str(new_dataset_id) + \"-train.csv\"\n",
    "        files_to_test = generate_test_dataset_list(dataset_path, dataset_name_std, dataset_id, path_in_name=True, ext=\"-test.csv\")\n",
    "\n",
    "        feature_selection_methods = generate_feature_selection_algorithms()\n",
    "        classifiers = create_classifiers()\n",
    "        scorers = create_scorers()    \n",
    "    \n",
    "        tests = TestingPipeline(None, files_to_test, y_function=device_class_anom_transform, name_in_path=True)\n",
    "        df = CustomDataSet(dataset_name_std + str(new_dataset_id) + \"-train.csv\", dataset_file, test_size=test_size, y_function=device_class_anom_transform)\n",
    "        run_classifiers(df, experimento, tests, scorers, classifiers, feature_selection_methods, refit=\"f1_score\")\n",
    "\n",
    "        final_ds = pd.concat([final_ds, tests.return_dataframe()])\n",
    "    final_ds.to_csv(\"./resultados/experimento-\" + experimento + \"/resultado_\" + \"experimento_\" + experimento + \"-\" + str(dataset_id) + \"-\" + today + \".csv\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
